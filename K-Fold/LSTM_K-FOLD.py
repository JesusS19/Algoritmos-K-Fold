# -*- coding: utf-8 -*-
"""
Created on Fri May 23 15:31:03 2025

@author: J. SaldaÃ±a
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold # Import KFold for K-Fold cross-validation
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout # Import Dropout for regularization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt # Import for plotting confusion matrix

# 1. Load the dataset
# The 'SubjectID' column is not needed for standard K-Fold splitting,
# but the code will read it if present.
df = pd.read_csv('Dataset_Marcha_acelerometroPX.csv')

# 2. Select relevant columns
X = df[['x', 'y', 'z']]  # Independent variables (features)
y = df['Clase']        # Target variable (class)

# 3. Scale the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 4. Convert data into time sequences (timesteps)
# The create_sequences function no longer needs the 'groups' column
def create_sequences(X, y, timesteps=10):
    X_seq, y_seq = [], []
    for i in range(len(X) - timesteps + 1):
        X_seq.append(X[i:i+timesteps])
        y_seq.append(y[i+timesteps-1])  # Label associated with the last timestep
    return np.array(X_seq), np.array(y_seq)

timesteps = 10
X_seq, y_seq = create_sequences(X, y, timesteps)

# 5. Implement K-Fold Cross-Validation
n_splits = 5 # Define the number of folds (e.g., 5 or 10 is common)
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True is crucial to mix data
                                                              # random_state ensures reproducibility

fold = 1
all_y_test = []
all_y_pred = []
all_accuracies = []
all_reports = [] # To store classification reports for each fold

# Iterate through the folds generated by KFold
for train_index, test_index in kf.split(X_seq, y_seq):
    print(f"\n--- Fold {fold}/{n_splits} ---")
    X_train, X_test = X_seq[train_index], X_seq[test_index]
    y_train, y_test = y_seq[train_index], y_seq[test_index]

    # 6. Create the LSTM model
    # The model is re-initialized for each fold to ensure independent training
    model = Sequential([
        LSTM(32, input_shape=(timesteps, X.shape[1]), return_sequences=False),
        Dropout(0.5),  # Add Dropout for regularization
        Dense(16, activation='relu'),
        Dropout(0.5),  # Add Dropout
        Dense(1, activation='sigmoid') # Sigmoid activation for binary classification
    ])

    # 7. Compile the model
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

    # 8. Train the model
    # You can adjust epochs as needed.
    history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2, verbose=0)

    # 9. Evaluate the model
    y_pred_fold = (model.predict(X_test) > 0.5).astype(int)
    fold_accuracy = accuracy_score(y_test, y_pred_fold)
    report_fold = classification_report(y_test, y_pred_fold, output_dict=True) # Get report as dictionary

    print(f"Accuracy for Fold {fold}: {fold_accuracy}")
    print(f"\nClassification Report for Fold {fold}:")
    print(classification_report(y_test, y_pred_fold)) # Print the full report

    all_y_test.extend(y_test)
    all_y_pred.extend(y_pred_fold)
    all_accuracies.append(fold_accuracy)
    all_reports.append(report_fold) # Store the fold's report

    fold += 1

print("\n--- Overall Results (Averaged Across Folds & Concatenated Predictions) ---")
print("Average Accuracy across folds:", np.mean(all_accuracies))
print("\nOverall Accuracy (from concatenated predictions across all folds):", accuracy_score(all_y_test, all_y_pred))
print("\nOverall Classification Report (from concatenated predictions):")
print(classification_report(all_y_test, all_y_pred))

# 10. Display the Confusion Matrix for the complete set of predictions
cm = confusion_matrix(all_y_test, all_y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(all_y_test))
disp.plot(cmap=plt.cm.Blues)
plt.title("Overall Confusion Matrix (K-Fold Cross-Validation)") # Title in English
plt.show()

# Optional: Display average metrics per class if desired
# from collections import defaultdict
# avg_metrics = defaultdict(lambda: defaultdict(float))
# for report in all_reports:
#     for class_name, metrics in report.items():
#         if isinstance(metrics, dict): # Ensure these are class metrics
#             for metric_name, value in metrics.items():
#                 if metric_name not in ['support']: # Do not average 'support'
#                     avg_metrics[class_name][metric_name] += value
#
# for class_name in avg_metrics:
#     for metric_name in avg_metrics[class_name]:
#         avg_metrics[class_name][metric_name] /= n_splits
#
# print("\n--- Average Class Metrics Across Folds ---")
# for class_name, metrics in avg_metrics.items():
#     print(f"Class {class_name}:")
#     for metric_name, value in metrics.items():
#         print(f"  {metric_name}: {value:.4f}")